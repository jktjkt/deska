\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[czech,english]{babel}
\usepackage{a4wide}

\begin{document}

\title{Deska: A Tool for Central Administration and Monitoring of a Grid Site}

\author{Jan Kundrát \and Martina Krejčová \and Tomáš Hubík \and Lukáš Kerpl}

\maketitle

\section{Background}

One of the activities taking place at the Institute of Physics of the~AS~CR (FZU)~\cite{fzu} is running a local Tier-2 computing
center connected to an international WLCG computing grid~\cite{wlcg}, supporting user programs from various scientific communities
from the whole world including the D-0 experiment in Fermilab~\cite{d0} and the famous LHC accelerator from CERN~\cite{lhc}.

\section{Motivation}

In December 2010, the total installed computing capacity available to users was about 3000~CPU cores with more than 500~TB of disk
storage, which translates to several hundreds of physical machines, with more hardware to arrive in early 2011.  All the resources
are managed by just a few system administrators (currently slightly more than three full time employees).  Due to the impressive
machines-per-administrator ratio, the staffers are exploring ways about how to make the system administrators' life easier and
less error prone.

Over the years, certain monitoring and management systems were deployed at the Institute, including Cfengine~\cite{cfengine},
Nagios~\cite{nagios}, Ganglia~\cite{ganglia}, Munin~\cite{munin} and various in-house monitoring appliances.  While these tools
certainly aim to make the sysadmin's life easier, introduction of each new tool also adds one more place for the administrator to
check if she wants to make sure the infrastructure works as expected, and each new tool has to be configured separately.
Therefore, it has been decided that the Institute shall fund a development of a tool which will {\em automate the configuration
process}.

\section{System Requirements}

\subsection{Database}

The aim of the first part of the project is to develop a database storing generic description of all the resources that the Grid
site is using; such a database should be capable of describing hardware infrastructure of the data center\footnote{That is all
physical machines, their hardware models, part numbers, rack locations, network interconnects, history of hardware failures etc},
couplings of operating system instances (aka ``hosts'') to physical machines, logical relations between various
services\footnote{For example, each {\em subcluster} has one or more {\em gateways} called Computing Elements.  Failure of the CE,
as detected by upstream monitoring, might be actually caused by a dying hard drive on one single node in the cluster.  The system
must therefore be capable of storing such complex information.}.
This database shall then act as the single authoritative source of information for generating configuration of all components of
the system, including the monitoring appliances.  In addition, tools shall be written to perform consistency checks between the
state recorded in the database and the actual system shape when the automatic configuration generation is not feasible for safety
reasons\footnote{A classic example of such a system are network switches -- system administrators are rather leery when it comes
to automated configuration as a single error can cause large network segments to separate.}.

Due to the Institute's administrators' strong Unix background and on their explicit request, it is expected that the interface to
this database will be provided via a CLI application instead of using a traditional web-based approach.  The main reason for this
design choice is their familiarity with text-based configuration tools and bad experience with web applications in general.  The
application interface shall be similar to a CLI-oriented interface used on enterprise-grade Ethernet switches, notable to the
Cisco IOS shell.  The format shall
enable storing of plaintext dumps of the database in a version control system.  It is expected that the CLI application will
include ``wizards'' for common tasks like deploying a new machine in order to make the user's job as smooth as possible.

The Database shall provide an API for access to the data contained therein.  User authentication and authorization shall be
handled by already existing operating system facilities (PAM, SSH).  The whole system shall be able to scale up to thousands of
nodes on a commodity x86\_64 server hardware.

Furthermore, additional components for generating configuration on basis of actual system state, as retrieved through the
read-only database API, shall be developed for all systems which are currently deployed at FZU's site, namely for Nagios, BIND
name service demon, DHCP server and Ganglia and Munin master servers.  The design of the API should reflect generic needs of any
data center to allow eventual deployment of new components, should the need arise.  The information available through the API
shall make it possible
to generate an accurate visualization of the data center and all deployed systems, from the physical view showing racks of servers
to a high-level view depicting logical dependencies between services and machines.

\section{Problems to Solve}

FIXME, FIXME, FIXME.

The structure of the Database shall be thoroughly planned, too.  The team will have to balance the complexity needed for accurate
representation of the real infrastructure with the required level of usability.  The interface should be intuitive enough so that
the administrator will actually notice the improvements and start to like the system in favor of manual hacks.

\section{Future Improvements}

The system shall be extensible enough to be used outside of the Institute's computing infrastructure.  A nice feature would be
also a ``one-click'' tool for fully automated deployment of new machines, handling everything from setting up PXE environment for
installation to actual system configuration.


\begin{thebibliography}{9}
    \bibitem{fzu}Institute of Physics of the AS CR home page, {\tt http://www.fzu.cz/}
    \bibitem{wlcg}The W-LCG LHC Computing Grid, {\tt http://lcg.web.cern.ch/LCG/}
    \bibitem{d0}The DZero Experiment, {\tt http://www-d0.fnal.gov/}
    \bibitem{lhc}The Large Hadron Collider, {\tt http://lhc.web.cern.ch/lhc/}
    \bibitem{cfengine}Cfengine, {\tt http://www.cfengine.org/}
    \bibitem{nagios}Nagios, {\tt http://www.nagios.org/}
    \bibitem{ganglia}Ganglia, {\tt http://ganglia.sourceforge.net/}
    \bibitem{munin}Munin, {\tt http://munin.projects.linpro.no/}
\end{thebibliography}

\end{document}
