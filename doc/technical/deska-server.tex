% vim: spelllang=en spell textwidth=120
\documentclass[deska]{subfiles}
\begin{document}

\chapter{Server part of Deska}
\label{sec:deska-server}

\begin{abstract}
This chapter goes into much detail in describing the general structure and operation of the server part of the Deska
system.  The discussed material covers the auxiliary Python application, the static stored procedures and the SQL code
generator.
\end{abstract}

\section{Server and its Database}
Server part of the Deska application can be divided into two parts, the {\em Deska server} and the {\em Deska database}.
The Deska server is a Python application which is started on-demand for each incoming connection, listens for the
commands from the {\tt deska-cli} console (see \secref{sec:dbapi-protocol} for the protocol description) and forwards
them to the stored procedures in the PostgreSQL database for further processing.  Some special functions, like the
\deskaFuncRef{freezeView} and the \deskaFuncRef{showConfigDiff}, are implemented directly in the Python server
itself.

Due to the dynamic nature of the user-defined database scheme (cf. \secref{sec:admin-dbscheme}) and strict requirements
of the PostgreSQL's type system, much of the stored procedures which actually transform the data are generated by the
SQL code generator during the initialization of the database (see \secref{sec:sql-generator}).  The interaction patterns
between the generated and static stored procedures is described in \secref{sec:deska-db-data-manipulation}.

\section{Deska Server}
\label{sec:server-py}

This section talks about the Deska server, a Python application wrapping access to the database.

\subsection{Command-line Options}

The {\tt deska-server} supports the following command-line options:

\begin{longtable}{ l | l }
    \caption{Command-line Options} \\
    Option & Meaning \\
    \hline
    \endhead
    {\tt --database} & Name of the PostgreSQL database to connect to \\
    {\tt --username} & User to connect to the DB \\
    {\tt --logfile} & Optional name of the file for a debug log \\
    {\tt --log-stderr} & Debugging option for sending the logs to a standard error \\
    {\tt --cfggen-backend} & Specify which configuration generator backend to use \\
    {\tt --cfggen-script-path} & Path to the directory with actual configuration generator scripts \\
    {\tt --cfggen-git-repository} & Path to the source Git repository clone (if using Git-based generators) \\
    {\tt --cfggen-git-workdir} & Path to use as a working directory for producing output \\
\end{longtable}

It is expected that these options are correctly set by a shell wrapper which starts the Deska server on the user's
behalf.

The {\tt deska-server} also supports determining these options from the environment variables.  For details and the list
of supported variable names, please see the \path{src/deska/server/app/deska_server_utils/server.py} file.

\subsection{Database connection}
After the Deska server starts, it tries to connect to the Deska database through the PsycoPg2 Python
module~\cite{psycopg}.  Parameters of connection are taken from Deska server parameters on start.  When the connection
is successfully established, it is kept until the Deska server is terminated.

\subsection{Command running}
For communication with the {\tt deska-cli}, the JSON-based DBAPI (see~\secref{sec:dbapi-protocol}) is used. On server
side, the incoming JSON data are transformed into Python's dicts and lists, and a few basic sanity checks are performed
(like the command being actually defined and allowed in the current context, and for an appropriate arguments).  Most of
the commands are actually implemented inside the database, so the Python server calls the appropriate database function
and returns the obtained result, catching and processing any exceptions which might have been raised.

Some commands are implemented directly in the {\tt deska-server}, though.  The \deskaFuncRef{freezeView} and
\deskaFuncRef{unFreezeView} simply sets up proper transaction control and session isolation, but there are two more
complicated exceptions, the \deskaFuncRef{showConfigDiff} and \deskaFuncRef{commitChangeset} which deal with running of
the configuration generators (much more information about the configuration details is available in chapter
\secref{sec:config-generators}).  These two functions, along with some assistance from the database through the {\tt
jsn.changesetHasFreshConfig} and {\tt jsn.markChangesetFresh} stored procedures, are responsible for maintaining proper
isolation between two concurrent generator runs, control the VCS interaction and generally drive the whole
output-producing logic.  For performance reasons, even the \deskaFuncRef{showConfigDiff} performs an artificial {\em
commit} behind the scenes so that the generators could access the data without an extra cost of history traversal.  In
order not to influence any other parallel sessions, the whole operation is wrapped inside a database transaction which
is automatically rolled back when the generators have finished running.

Finally, the last exception is the \deskaFuncRef{applyBatchedChanges} DBAPI function which is used to efficiently
execute multiple operations in an atomic manner.  This function again relies on the underlying database transactions,
and hence requires a small amount of custom code at the Python server layer.

\subsection{Security Model}
\label{sec:server-security-model}

The Deska server relies heavily on the authentication/authorization features offered by the SSH dameon and on the
{\tt trust = ident} feature of the PostgreSQL server.  Any Unix user who is able to connect to the PostgreSQL database
which contains the Deska installation will be able to perform any modification to the database over the standard DBAPI
calls.  In future, the server layer could be easily instrumented with command hooks delegating the authorization to an
external component on a command-by-command basis.

\subsection{Exception Handling}

Execution of the server-side methods can, obviously, cause errors.  We are handling these situations through exceptions
which are reported back to the caller over the {\tt dbException} JSON argument (see \secref{sec:dbapi-exceptions}).
General errors in the execution or problems with the database connection are handled in the same manner.

\section{Deska Database}

The Deska {\em database}, an application written for the PostgreSQL~\cite{postgresql} database server, stores the user
data along with its history and is tasked with the general data processing and manipulation duties.

\subsection{Data Storage}
To prevent a possible misunderstanding when talking about the database, the word {\em scheme} can be used in two
distinct meanings.  The {\em user-defined} scheme refers to various tables and constraints which together describe the
data that a user can put to the database, while the second meaning of the word refers to the PostgreSQL's {\tt schema}
stanza, a construct similar to C++'s namespaces that simply puts related database objects (from tables and types to
functions) together.  We will try hard to make the difference clear throughout this chapter, always referring to the
user-defined scheme with the {\em user-defined} prefix and using the word variant {\em scheme}, while suffixing the
PostgreSQL-level schemas with their concrete names and calling each of them a {\em schema}.

Lets look at how data are stored in Deska database. Three types of data are stored in the Deska database.

\subsubsection{User-defined Scheme}
Firstly there are information about the user defined database scheme.  These information are provided by the
\deskaFuncRef{kindNames}, \deskaFuncRef{kindAttributes} and \deskaFuncRef{kindRelations} DBAPI functions.  The required
data are determined from the database catalogue at the installation time and stored for future use in the
\path{generated.py} Python file for performance reasons, so that the database catalogue does not have to be accessed all
the time.

\subsubsection{Objects Stored in Deska Database}

The second important piece of information stored in the database are the user data.  As we have already mentioned, the
objects are stored as rows in various tables.  After a changeset is started, all data inserted into the database are
placed into tables in the {\tt history} schema. Each and every kind therefore has its own table in the {\tt history}
schema, with all attributes defined in the user defined scheme (and some more for history tracking).  Tables defined by
the user are placed into the schema {\tt production}. These tables (along with their constraints) are used for data
validation when committing changeset. If data are successfully copied into tables in schema {\tt production} from tables
in the {\tt history} schema, data are valid against the user-defined constraints and the commit shall therefore proceed.
See \secref{sec:versioning} for more info.

\subsubsection{Revisions and Changesets}
The final, third class of data stored in the database is information about revisions and changesets. There are two
tables, {\tt version}\footnote{The name {\tt version} was chosen before the Deska nomenclature settled on using the word
{\em revision}, which might be slightly non-intuitive.} and {\tt changeset}. These tables are placed in the {\tt
versioning} schema.

\subsection{Data Manipulation}

The Deska DBAPI (\secref{sec:dbapi-protocol}) mandates a number of functions that somehow manipulate user data.  In
Deska, these functions are implemented through PostgreSQL's stored procedures.  Because these functions have to be
properly statically typed, they are generated at the (user-defined) scheme deployment time through the SQL generator
(see \secref{sec:sql-generator}) and are stored in the {\tt genproc} schema.

Functions for version control, changeset/revision management etc. are written by hand and can be found, among many
hand-written functions, in the {\tt deska} schema.

\subsection{Version control}
For each data modification it is necessary to start changeset and do modifications in context of some open changeset. New changeset could be open by calling startChangeset function. This function inserts row into changeset table with information about author, parent revision of the new changeset and pid. Pid is process id of connection to the Deska database. Through pid is later recognized with which temporary changeset the given connection works and in which temporary changeset should be the modification stored. Each connection could have attached at most one temporary changeset.\\
Temporary changesets could be closed by calling commitChangeset function. This function closes temporary changeset that is attached to the connection and creates new version. Closing of the changeset is done by deleting of row with record about it from the changeset table. New version is created by inserting a new row into version table. The row has information about id of the new version, author, id of the changeset from which was this new version created and commit message.\\
Temporary changeset could be closed also by calling abortCurrentChangeset. This function just closes the attached temporary changeset.\\
It is also possible to just detach from current changeset by deleting the information about pid from the row with attached changeset. Or attach to any temporary changeset by setting pid attribute of appropriate row.\\
Stored procedures, which ensure version control, are placed in schema deska, tables are in schema versioning.

\subsection{Communication}
\begin{figure}[h]
	\centering
	\label{img:deska-server}
	\includegraphics[trim=28mm 170mm 30mm 28mm]{img-deska-server-components.pdf}
	\caption{Deska server components}
\end{figure}

All communication with Deska server is implemented in schema json. There are functions, written in pg-python, which use another
functions, to manipulate with data in database. Or get data directly from tables as in {\tt pendingChangeset} and {\tt listRevisions}
\footnote{This direct access is not shown in the picture}.
Why we created this layer, and why it returns JSON data and not table or normal value, will be described in \secref{sec:schema-json}

The picture \ref{img:deska-server} shows how Deska server and Deska database communicate. Schema json provides interface for Deska server. It
calls functions from other schemas (genproc and deska). All these schemas are drawn as components. Schemas containing
tables are shown in different form, because they do not provide real interface.

\subsection{Json schema}
\label{sec:schema-json}
After we had generated functions for manipulating with data, and DBAPI specified, it was clear, that there must be some layer,
that transforms createObject(kindName,xxx) to kindName\_add(xxx), which is call of generated SQL function.
We do that in Deska database, because we wanted to have 1:1 mapping between database functions, and DBAPI commands.

It worked well until first performance testing was made. The state was, that Deska server called for data - millions of objects,
database worked and worked and... when finally returned this data to server, server started transformation... It took long time
and consumed lots of memory.
In the same time, diffing implementation started, and there where problems with many functions of different result types,
that has to be called, which was hard to union into one function.
So there was an idea, that JSON can be created inside database, so that the union of diffing function can return JSON.

Some test were run \secref{sec:test-json}, the time for smaller data was worse with JSON in database, but for large data there was significant
speed up. The function written in pg-python creates part of the JSON structure for each line of database cursor.
So the JSON structure grows up with data that database produced.

The result is, that every function callable by server returns JSON.

\subsection{Getting data from tables}
\label{sec:data-functions}
Here we talk about getting data of objects, revisions and changesets stored in tables.
These data are get from tables, or table returning functions for get data in proper version.
See \secref{sec:versioning} how these functions work. Here we talk about functions in json schema.

There are {\tt pendingChangeset} that selects data from changeset table, and {\tt listRevisions} that selects data from version table.
Both accept filter as parameter and use some cast optimization, about which we will talk in section about cast
optimization \secref{sec:cast}.

Object data functions {\tt objectData}. {\tt resolvedObjectData}. {\tt resolvedObjectDataWithOrigin} call generated functions 
that return tuple of attribute values, that is just converted into JSON format and returned. 

In functions {\tt multipleObjectData}. {\tt multipleResolvedObjectData} and {\tt multipleResolvedObjectDataWithOrigin}
more work is done than in another json schema function, that only runs some function.
There is still only one main call another database function (and one helping, to get revision number).

But the SQL statement to run is created in more difficult way. In contrast to other json schema functions,
here we create complicated {\em column definition} for the SQL statement. It goes between SELECT and FROM keywords.
This column definition contains definition of all columns that shall be returned in human readable form.
In database tables, reference columns has bigint type, but user wants to see text name of referenced kind.
The sets are represented by another table, but user wants to see array etc. So some transformation must be done.
It is the easiest for simple attribute,
for example if kind host has attribute note, than the column definition is {\em host.note}.
It gets complicated, when the attribute is part of some relation. For example refers\_to.
If host has attribute hardware, referencing to kind hardware. Than column definition is 
{\em hardware\_get\_name(host.hardware,\$1)}. For sets, templates and embed kinds it is similar.

\subsubsection{Example}
Lets see the {\tt multipleResolvedObjectDataWithOrigin} function to explain, how it work. The other functions are very similar.
%which name is set???
At first, name is set (this is used for result JSON and/or reporting errors) and JSON structure is prepared,
using dutil.jsn function. After this, kindName is checked. Then attributes are load from generated.py file.
If kind has template, every attribute except the one, which starts with template\_ prefix, is duplicated, so that we have
also information about origin of the value.
Then columns definition is generated. For normal attribute, "kindName.attributeName" is the definition.
But for referring attributes, which are
represented as uid in database, not string, kindName\_get\_name function must be called. So in the case that attribute vendor is represented as id, then
"vendor\_get\_name(kindName.attributeName, version) AS kindName.attributeName" is the column definition. For embedded kind, embed name is created in similar way,
using function join\_with\_delim.
Then, the final SQL statement is created using function dutil.getSelect, columns definition and JOIN and WHERE parts from filter (see \secref{sec:filters}) are used.
This function also checks if this kind has template or not, and proper function resolved or not is used in the final SQL statement.

After this preparation, SQL statement is run and the result is fetched. For each line in the result table, one part of result JSON structure is
created. And then final JSON is created and returned.

And here is some example of SQL statement which is called:
\begin{minted}{sql}
SELECT host.service_templ --simple column definition
,host.name,host.service,host.virtual_hardware_templ,
--column definition for uid column of template
host_template_get_name(host.template_host,$1) AS host_template
--column definition for uid column of reference
,hardware_get_name(host.hardware,$1) AS hardware,
host.note_host_templ,virtual_hardware_get_name(host.virtual_hardware,$1) AS virtual_hardware,
host.hardware_templ,
host.note_host FROM
host_resolved_data_template_info($1) AS host -- call function for get resolved data with template info
LEFT OUTER JOIN hardware_resolved_data($1) AS hardware ON host.uid = hardware.host -- join host kind
WHERE hardware.name != $2 OR hardware.name IS NULL -- perform filter condition
\end{minted}

\subsection{Direct access to resolved data}
\label{sec:direct-access}
If you are not in changeset, and do not give revision parameter to function {\tt multipleResolvedObjectData},
you will access directly to tables, because resolved data are in production schema tables.

This is used by configuration generators and it is much more faster to read
resolved data than do this in history schema, where complecated computation is run.
See test results in \secref{sec:test-direct}

\subsection{Getting data from tables - casting}
\label{sec:cast}
For performance reasons we do one more thing in column definition. It is casting.
In pg-python extension, there is a function that from Postgresql types create native python types.
It is called Postgres.convert\_postgres\_objects
and must be called for every result from database. But it supports only some types.
Some of these types are converted in different way than we want.

At first, we solved this with our own function, that did the transformation, but if it ran for every
row and every column, it was very slow. So the most of these transformations is now done using Postgresql cast
in SQL statements. It is much faster and saves memory (see \secref{sec:test-cast}

The column definition is more complicated than we describe above. If
Postgres.convert\_postgres\_objects function 
does not support conversion of given column's type, or do it in bad way for us, we add cast in column definition.
For example if there is timestamp attribute, we add {\em ::text}, if it is ipv4 type, and we don't want
the conversion with mask suffix, we use {\em host} function in column definition.


\subsection{Filters}
\label{sec:filters}
Some functions (lets call them {\em data functions} in json schema have parameter filter. It is in JSON format,
see DBAPI protocol in~\secref{sec:api-filters} for more info.
The data function, in json schema, have part where SQL statement is created. It is described in
\secref{sec:data-functions}.

This filter is implemented in filter.py file in class {\tt Filter}.
It adds JOIN and WHERE parts to the SQL statement that is used by data functions. The {\tt Filter} class has
methods getJoin and getWhere that return strings which fit at the end of SQL statement in data function.

Filter constructor parses given filter into parts - conditions.
Every condition has kind, attribute, operator and value. From this we create strings line
{\em kind.attribute = kind2\_get\_uid(\$2)}. If there are more conditions, they are joined with AND or OR
operators.
If kind in condition is different from kindName - function parameter, it is remembered,
and in getJoin function, if it is possible, string like {\em JOIN kind2 ON kind2.uid = kind.attribute}
is returned. This is possible when some relation between these kinds exists.
If there are more different kinds, join string with more JOINs is produced.
Information about relation are taken from generated.py file.

It was added into Deska later than other functions, so that it uses what was developed before.

\subsection{Filters limitations}
\label{sec:filter-speed}
As we have said, filtering requirement was add very late. In that time, data functions has parameter revision
and only way to implement this is via function returning table.

So the filter is applied on the result of this function, providing parametrized view.
Except in {\em multipleResolvedObjectData}, which access directly to the tables, when
you are not in changeset and revision parameter is not set.

But main problem is not, what to select at first. It is that table returning function has
no indexes and every filtering is done by full table scan.
This slows filters down, and it must be improved in the future. It is not hard,
if you use view instead of function simulating parametrized view, indexes can be used.
And as we can see, revision parameter is not used now, so if DBAPI specification changes,
we can transform this functions into views. 

\subsection{Error handling}
Here we try to explain work with exceptions in Deska database and Deska server. Postgresql functions can RAISE exception with given message and sqlstate.
This sqlstate is like number of error. We use this to split possible exceptions into some categories - exception types. Exception in database is
every unexpected or “bad” thing that occurs. It can be runtime error as well as constraint violation, or exception thrown explicitly by RAISE.

While json schema provides JSON as return type, we try to catch maximum number of exceptions inside the database (in
json schema part).
For this catching DeskaException class is used. It has constructor, with Postgresql.dberr as parameter, which structure is that contains exception
message, and sqlstate. We use this information to determine exception type - class contains typeDict - dictionary {sqlstate: exceptionType}.
%don't understand function has json method
Finally this function has json method, which takes name on the function, in which error occurs and creates exception in JSON format.

\label{sec:deska-db}

\section{Data manipulation in Deska database}
\label{sec:deska-db-data-manipulation}
In this section you can find how the Deska database works, how the data versioning, templating and other Deska's features are done.\\
Almost all tables and stored procedures are generated by sql-generator described in \secref{sec:sql-generator}.

\subsection{Versioning}
\label{sec:versioning}

The database can return object's data in any version in which it exists. This could be done due to history tables that are generated for each table defined by user.\\
Each history table contains, in addition to attributes in the table defined by user, attributes {\tt version} and {\tt dest\_bit}. In version attribute is information about the changeset in which the object was modified. {\tt dest\_bit} attribute set to 1 means that the object is deleted, default value is 0.\\

\subsubsection{Data Modification}
\label{sec:deska-db-ver-data-modif}

\begin{longtable}{ l | l | l | l | l | l }
    \caption{Database table {\tt hardware\_history}}\\
    name & uid & purchase & note & dest\_bit & version\\
    \hline
    \endhead
\label{tab:example-hardwarehist}
    h1 & 1 & & & 0 & 1 \\
    h2 & 2 & & & 0 & 1 \\
    h2 & 2 & & note & 0 & 2 \\
    h2 & 2 & 01-01-2011 & note & 0 & 3 \\
    h1 & 2 & & & 1 & 3 \\
    \hline
\end{longtable}

In table \ref{tab:example-hardwarehist} is example of hardware's history table. This is how the hardware\_history table looks like after following operations. 
\begin{itemize}
    \item In changeset 1 were inserted hardware h1, h2.
    \item In changeset 2 was h2's note attribute set to note.
    \item In changeset 3 was h2's purchase attribute set to 01-01-2011 and hardware h1 was deleted.
\end{itemize}


If the table \ref{tab:example-hardwarehist} was only a piece of data from hardware\_history table then we could not recognize if these objects were in changeset 1 created or some of their attributes were just set.\\
In row with hardware h2, version 3 we can see that when object is modified, row with its actual data and id of current changeset is inserted into history table, after that the attribute which has to be modified is set.\\
In words of stored procedures it could be written so:\\

\begin{minted}{sql}
select startchangeset();
select hardware_add('h1');
select hardware_add('h2');
select commitchangeset('.');

select startchangeset();
select hardware_set_note('h2','note');
select commitchangeset('.');

select startchangeset();
select hardware_set_purchase('h2','01-01-2011');
select hardware_del('h1');
select commitchangeset('.');
\end{minted}

\subsubsection{Retrieve Object Data}

If we would like to get data of some objects in given version. We have to find the objects' last modification. To find the really last objects' modification, we need to know version table.\\
At the time of changeset commit, table with versions is updated. For our example it could looks like \ref{tab:example-version}.\\
Important information for us are id and num. id is id of changeset from which the version num was created. The version number num is an increasing sequence, if version num2 was created on the base of version num1 then num1 < num2.

\begin{longtable}{ l | l | l | l | l }
    \caption{Database table {\tt version}}\\
    id & num & author & timestamp & message\\
    \hline
    \endhead
\label{tab:example-version}
    0 & 1 & martina & 2011-12-17 17:04:29.56794 & Initial revision\\
    1 & 2 & martina & 2011-12-17 17:04:47.876152 & .\\
    2 & 3 & martina & 2011-12-17 17:04:54.731837 & .\\
    3 & 4 & martina & 2011-12-17 17:05:12.489788 & .\\
    \hline
\end{longtable}


There are some examples of data retrieving, results are in table \ref{tab:select-hardware} and table \ref{tab:select-hardware-v3}.\\

\begin{minted}{sql}
--functions for data retrieving without parameters returns the most actual data
select * from hardware_data_version();
select * from hardware_data_version(3);
\end{minted}

\begin{longtable}{ l | l | l | l | l | l }
    \caption{Database table {\tt hardware\_data\_version()}}\\
    uid & name & note & purchase & version & dest\_bit\\
    \hline
    \endhead
\label{tab:select-hardware}
    2 & h2 & note & 2011-01-01 & 3 & 0\\
    \hline
\end{longtable}

\begin{longtable}{ l | l | l | l | l | l }
    \caption{Database table {\tt hardware\_data\_version(3)}} \\
    uid & name & note & purchase & version & dest\_bit\\
    \hline
    \endhead
\label{tab:select-hardware-v3}
    1 & h1 &  &  & 1 & 0\\
    2 & h2 & note &  & 2 & 0\\
    \hline
\end{longtable}

\subsubsection{Commit}
Commit function selects objects from history table of given kind which were just added, deleted or updated in the current changeset. Rows that belong to added objects are inserted into given kind's table in the schema production. Rows that belong to deleted objects are deleted from kind's table in the schema production. And rows that belong to updated objects are updated in kind's table in the schema production.\\
Commits from our example will cause these changes:
\begin{itemize}
    \item After changeset 1 rows with hardware h1, h2 will be inserted.
    \item After changeset 2 h2's note attribute will be updated.
    \item After changeset 3 h2's purchase attribute will be updated and hardware h1 will be deleted.
\end{itemize}

\subsection{Refers To}
\label{sec:db-refs-to}
If the table is in the {\tt REFERS TO} relation (see \secref{sec:relation-refers-to}) with another, it has to have column that refers to the {\tt uid} column of the another table. This column, which refers to the {\tt uid} column of another table, we will call here {\em refuid}.\\
Stored procedures for setting refuid columns expect as parameter some value of name attribute from referenced table. The given name is found in referenced table and proper {\tt uid} is set as value of refuid column instead of the name.

Here is an example of setting refuid column. The result of these selects is in tables \ref{tab:refs-hardware} and  \ref{tab:refs-vendor}.
\begin{minted}{sql}
select startchangeset();
select vendor_add('v1');
select vendor_add('v2');
select hardware_add('h1');
select hardware_set_vendor('h1','v2');
select commitchangeset('.');

select * from hardware;
select * from vendor;
\end{minted}

\begin{longtable}{ l | l | l | l | l }
    \caption{Database table {\tt hardware}}\\
    uid & name & note & purchase & vendor\\
    \hline
    \endhead
\label{tab:refs-hardware}
    1 & h1 &  &  & 2\\
    \hline
\end{longtable}

\begin{longtable}{ l | l }
    \caption{Database table {\tt vendor}}\\
    uid & name \\
    \hline
    \endhead
\label{tab:refs-vendor}
    1 & v1\\
    2 & v2\\
    \hline
\end{longtable}


\subsection{Object Embedding}
Object embedding is described in \secref{sec:relation-embed-into}. The object which is embedded into another object could be identified by the global name, pair of object's local name and parent's name. Embedded objects have an attribute which refer to the parent object's uid. With this attribute is manipulated in the same way as with the refuid attributes described in \secref{sec:db-refs-to}.\\
The following example shows how to work with embedding, the table identifier is embedded into the table host. Result of these selects is in the tables \ref{tab:embed-host} and \ref{tab:embed-interface}.

\begin{minted}{sql}
select startchangeset();
select host_add('h1');
select host_add('h2');
select interface_add('h1->i1');
select interface_add('h1->i2');
select interface_add('h2->i1');
select commitchangeset('r2');
\end{minted}

\begin{longtable}{ l | l }
    \caption{Database table {\tt host}}\\
    uid & name \\
    \hline
    \endhead
\label{tab:embed-host}
    1 & h1\\
    2 & h2\\
    \hline
\end{longtable}

\begin{longtable}{ l | l | l | l }
    \caption{Database table {\tt interface}}\\
    uid & name & host & note\\
    \hline
    \endhead
\label{tab:embed-interface}
    1 & i1 & 1 & \\
    2 & i2 & 1 & \\
    3 & i1 & 2 & \\
    \hline
\end{longtable}


\subsection{Templates}
{\tt TEMPLATIZED} relation which is described in \secref{sec:relation-templatized} is implemented by template tables. 

\subsubsection{Value inheritance}
The columns, which are a part of some {\tt COMPOSITION} relation or {\tt EMBED INTO} relation, can not inherit its value from templates. Therefore these columns are not present in the template table. All other attributes from the templated table are also in the template table.\\
As the template tables are templated by themself, object can inherit value of its attributes transitively from more templates.\\
To ensure right data retrieving from given version, template tables need to have history tables as well.\\
Tables in the schema production store resolved data.\\
The following example shows how we can work with the templated tables. The hardware table is templated by the hardware\_template table. The result of these selects is in tables \ref{tab:templ-hwhist} and \ref{tab:templ-hwtemplhist}.

\begin{minted}{sql}
select startchangeset();
select hardware_template_add('t1');
select hardware_template_set_note('t1','note from template t1');
select hardware_template_set_purchase('t1','01-01-2011');
select hardware_add('h1');
select hardware_set_template_hw('h1','t1');
select commitchangeset('r2');

select startchangeset();
select vendor_add('v1');
select hardware_template_add('t2');
select hardware_template_set_vendor('t2','v1');
select hardware_template_set_purchase('t2','01-01-2011');
select hardware_template_set_template_hw('t1','t2');
select commitchangeset('r3');

select startchangeset();
select hardware_set_note('h1','own h1 note');
select commitchangeset('r4');
\end{minted}

\begin{longtable}{ l | l | l | l | l | l | l | l }
    \caption{Database table {\tt hardware\_history}}\\
    uid & name & note & purchase & vendor & template\_hw & version & dest\_bit\\
    \hline
    \endhead
\label{tab:templ-hwhist}
    1 & h1 &  &  &  & 1 & 1 & 0\\
    1 & h1 & own h1 note &  &  & 1 & 3 & 0\\
    \hline
\end{longtable}

\begin{longtable}{ l | l | l | l | l | l | l | l }
    \caption{Database table {\tt hardware\_template\_history}}\\
    uid & name & note & purchase & vendor & template\_hw & version & dest\_bit\\
    \hline
    \endhead
\label{tab:templ-hwtemplhist}
    1 & t1 & note from template t1 & 2011-01-01 &  &  & 1 & 0\\
    2 & t2 &  & 2011-01-01 & 1 &  & 2 & 0\\
    1 & t1 & note from template t1 & 2011-01-01 &  & 2 & 2 & 0\\
    \hline
\end{longtable}


By calling the following commands we can get resolved data, it means with inherited values.\\
It is possible to get resolved data even with the data origin, this function is introduced in subsection about multi-value references \secref{sec:multi-val}.
\begin{minted}{sql}
--from version 'r3'
select * from hardware_resolved_data(3);
select * from hardware_template_resolved_data(3);
\end{minted}

Results are in \ref{tab:templ-hwres2} and \ref{tab:templ-hwtemplres2}.

\begin{minted}{sql}
--in production are the most actual resolved data
select * from hardware;
select * from hardware_template;
\end{minted}
Results are in \ref{tab:templ-hw} and \ref{tab:templ-hwtempl}.

\begin{longtable}{ l | l | l | l | l | l | l }
    \caption{Database table {\tt hardware\_resolved\_data(3)}}\\
    uid & name & note & purchase & vendor & template\_hw & dest\_bit\\
    \hline
    \endhead
\label{tab:templ-hwres2}
    h1 & 1 & note from template t1 & 2011-01-01 & 1 & 1 & 0\\
    \hline
\end{longtable}

\begin{longtable}{ l | l | l | l | l | l | l }
    \caption{Database table \\ {\tt hardware\_template\_resolved\_data(3)}}\\
    uid & name & note & purchase & vendor & template\_hw & dest\_bit\\
    \hline
    \endhead
\label{tab:templ-hwtemplres2}
    t2 & 2 &  & 2011-01-01 & 1 &  & 0\\
    t1 & 1 & note from template t1 & 2011-01-01 & 1 & 2 & 0\\
    \hline
\end{longtable}

\begin{longtable}{ l | l | l | l | l | l }
    \caption{Database table {\tt hardware}}
    \label{tab:templ-hw} \\
    uid & name & note & purchase & vendor & template\_hw\\
    \hline
    \endhead
    1 & h1 & own h1 note & 2011-01-01 & 1 & 1\\
    \hline
\end{longtable}

\begin{longtable}{ l | l | l | l | l | l }
    \caption{Database table {\tt hardware\_template}}\\
    uid & name & note & purchase & vendor & template\_hw\\
    \hline
    \endhead
\label{tab:templ-hwtempl}
    1 & t1 & note from template t1 & 2011-01-01 & 1 & 2\\
    2 & t2 &  & 2011-01-01 & 1 & \\
    \hline
\end{longtable}

\subsubsection{Commit of Templated Tables}
As data in the production schema are resolved, it is needed during commit find modified objects and resolve their data. We should keep in mind that object's data could be modified even by modification in the object that templates this object.\\
Commit of templated table is composed of:
\begin{enumerate}
    \item find all objects in the template table that were modified in the current changeset
    \item find all objects in the template table that can transitively inherit data from objects in step 1
    \item find all objects in the templated table that were modified or inherit data from objects in step 2
    \item resolve data of objects from step 3
    \item process data from step 4 as it was selected from history table of not templated table
\end{enumerate}

\subsection{Diff}
Modifications which were done between two versions or inside a temporary changeset could be listed by diff functions. Diff has these stages:
\begin{enumerate}
    \item init diff
    \item process diff data
    \item terminate diff
\end{enumerate}
Stage init diff creates temporary table with almost all data needed to get list of all modifications.
More init functions exist, one for diff between two versions and one for diff between state in temporary changeset and in its parent version. For templated kinds these init diff functions exist even in resolved modification.\\
Temporary table created in init diff stage could be then processed by diff functions. Each type of "basic modification" described in DBAPI \secref{sec:dbapi-protocol} is listed by separate diff function. Basic modifications setting a single attribute are listed by one function for all attributes of {\tt identifier\_set} type and next one for the rest of attributes.\\
The example below shows how you can select changes between versions.

\begin{minted}{sql}
select startchangeset();
select hardware_add('h1');
select hardware_add('h2');
select hardware_set_purchase('h1','02-12-2011');
select commitchangeset('r2');

select startchangeset();
select hardware_set_purchase('h1','01-01-2012');
select hardware_add('h3');
select hardware_del('h2');
select hardware_set_name('h1','hr');
select commitchangeset('r3');

--init
select hardware_init_diff(2,3);

--process diff data
select * from hardware_diff_created();
--returns h3

select * from hardware_diff_deleted();
--returns h2

select * from hardware_diff_rename();
--returns oldname:h1   newname:hr

select * from hardware_diff_set_attributes(2,3);
--returns objname:hr   attribute:purchase   olddata:2011-02-12   newdata:2012-01-01

--terminate
select hardware_terminate_diff();
\end{minted}

\subsection{Composition}
The {\tt COMPOSITION} relation is described in \secref{sec:relation-contains}.
Linking objects with the same name from different tables is maintained by before and after insert and update triggers. These triggers ensure even disjoining if one of linked objects is renamed or deleted and can immediately join renamed object with another one.\\
Condition that one object could be contained in at most one another object is checked by the check constraints. When the table represents kind that is containable to more than one kind, the containable table has this check constraint.\\
Following example shows, how the objects in {\tt COMPOSITION} relation are linked and disjoint. Result is in tables \ref{tab:comp-box}, \ref{tab:comp-hw} and \ref{tab:comp-switch}.

\begin{minted}{sql}
select startchangeset();
select hardware_add('h1');
select hardware_add('h2');
select hardware_add('h3');
select box_add('h1');
select box_add('h3');

--select switch_add('h1');
--would raise exception (violates check constraint) - h1 switch and h1 hardware can not contain the same box
select box_add('o1');
select commitchangeset('r2');

select startchangeset();
--renamed hardware is immediately linked with box with the same name
select hardware_set_name('h2','o1');
select hardware_del('h3');
--box h3 is now free, switch h3 could contains box h3
select switch_add('h3');
select commitchangeset('r3');

select uid, name, box from hardware;
select uid, name, hardware, switch from box;
select uid, name, box from switch;
\end{minted}

\begin{longtable}{ l | l | l | l }
    \caption{Database table {\tt box}}\\
    uid & name & hardware & switch\\
    \hline
    \endhead
\label{tab:comp-box}
    1 & h1 & 1 & \\
    3 & o1 & 2 & \\
    2 & h3 &  & 1\\
    \hline
\end{longtable}

\begin{longtable}{ l | l | l }
    \caption{Database table {\tt hardware}}\\
    uid & name & box\\
    \hline
    \endhead
\label{tab:comp-hw}
    1 & h1 & 1\\
    2 & o1 & 3\\
    \hline
\end{longtable}

\begin{longtable}{ l | l | l }
    \caption{Database table {\tt switch}}\\
    uid & name & box\\
    \hline
    \endhead
\label{tab:comp-switch}
    1 & h3 & 2\\
    \hline
\end{longtable}


\subsection{Multi-value References}
\label{sec:multi-val}
Multi-value references are described in \ref{sec:relation-multi-value-references}.\\
For each table's attribute, which is a part of some multi-value reference, is generated table, we call it {\em inner table}. This inner table stores for each object from the table its {\tt identifier\_set} value.\\

\subsubsection{Data Modifications and Versioning}
As inner table stores data of attribute that is part of versioned data, it is needed to have history table for all inner tables as well.\\ 
In addition to ordinary set attribute functions {\tt identifier\_set} attributes could be modified also by functions that insert or remove identifier into or from identifier set.\\
Each inner table has attribute flag that is used to distinguish between rows representing empty set from rows representing NULL value.\\
Following example shows how we can work with {\tt identifier\_set}s. In tables \ref{tab:multi-hosthist} and \ref{tab:multi-innerhist} you can see whole history of objects in the example. The table \ref{tab:multi-hostv} shows result of the last select, it selects data from given version.

\begin{minted}{sql}
select startchangeset();
select service_add('s1');
select service_add('s2');
select service_add('s3');
select host_add('h1');
select host_add('h2');
select host_set_service('h1',ARRAY['s1','s2']);
select commitchangeset('r2');

select startchangeset();
--we can remove just one item from identifier_set
select host_set_service_remove('h1','s2');
--we can insert just one item to identifier_set
select host_set_service_insert('h2','s3');
select commitchangeset('r3');

select startchangeset();
--host h1 has now empty set of services
--flag is still 1
select host_set_service_remove('h1','s1');
select commitchangeset('r4');

select startchangeset();
--flag is now 0
select host_set_service('h1',NULL);
select commitchangeset('r5');

select * from host_history;
select * from inner_host_service_history;
--this is how you can get hosts' services in given version
select uid, name, host_get_service(uid,4) from host_data_version(4);
\end{minted}

\begin{longtable}{ l | l | l | l }
    \caption{Database table {\tt host\_history}}\\
    uid & name & dest\_bit & version\\
    \hline
    \endhead
\label{tab:multi-hosthist}
    1 & h1 & 0 & 1\\
    2 & h2 & 0 & 1\\
    1 & h1 & 0 & 2\\
    2 & h2 & 0 & 2\\
    1 & h1 & 0 & 3\\
    1 & h1 & 0 & 4\\
    \hline
\end{longtable}

\begin{longtable}{ l | l | l | l }
    \caption{Database table {\tt inner\_host\_service\_history}}\\
    host & service & flag & version\\
    \hline
    \endhead
\label{tab:multi-innerhist}
    1 & 1 & 1 & 1\\
    1 & 2 & 1 & 1\\
    1 & 1 & 1 & 2\\
    2 & 3 & 1 & 2\\
    1 &  & 1 & 3\\
    1 &  & 0 & 4\\
    \hline
\end{longtable}

\begin{longtable}{ l | l | l }
    \caption{Database table {\tt host}}\\
    uid & name & service\\
    \hline
    \endhead
\label{tab:multi-hostv}
    1 & h1 & \{\}\\
    2 & h2 & \{s3\}\\
    \hline
\end{longtable}

\subsubsection{Templates}
The table, to which attribute an inner table is made, could also be templated. So it is needed to have template table of these inner tables and have functions for resolving templated inner table's data.\\
Following example shows how we can work with templated {\tt identifier\_set}s. Promised example of function, which retrieves resolved objects' data with the data origin, is introduced in the example. Also the function which processes diff data and lists data modifications of {\tt identifier\_set} attributes is showed in the example.\\

\begin{minted}{sql}
select startchangeset();
select service_add('s1');
select service_add('s2');
select service_add('s3');
select host_add('h1');
select host_add('h2');
select host_template_add('t1');
select host_template_add('t2');
select host_template_set_service('t2',ARRAY['s1','s2']);
select host_template_set_template_host('t1','t2');
select host_template_set_note_host('t1','note from t1');
select host_set_template_host('h1','t1');
select host_set_template_host('h2','t2');
select commitchangeset('r2');

select startchangeset();
select host_set_service_remove('h2','s2');
select host_template_set_service_insert('t1','s3');
select commitchangeset('r3');

select host_init_resolved_diff(2,3);
select * from host_diff_refs_set_set_attributes(2,3);
--returns these two rows
--objname:h1 attribute:service olddata:{s1,s2} newdata:{s1,s2,s3}
--objname:h2 attribute:service olddata:{s1,s2} newdata:{s1}

select host_terminate_diff();
\end{minted}

By calling the following selects we can get resolved data with the data origin, it means with inherited values and name of the object from which was the value taken.\\
\begin{minted}{sql}
select * from host_resolved_data_template_info();
select * from host_resolved_data_template_info(2);
\end{minted}

Results are in tables \ref{tab:multi-reshost} and \ref{tab:multi-reshostv2}.

\begin{longtable}{ l | l | l | l | l | l | l }
    \caption{Database table {\tt host\_resolved\_data\_template\_info()}}\\
    uid &  name &  service &  service\_templ &  note\_host &  note\_host\_templ &  template\_host\\
    \hline
    \endhead
\label{tab:multi-reshost}
    2 & h2 & \{s1\} & h2 &  &  & 2\\
    1 & h1 & \{s1,s2,s3\} & t1 & note from t1 & t1 & 1\\
    \hline
\end{longtable}

\begin{longtable}{ l | l | l | l | l | l | l }
    \caption{Database table {\tt host\_resolved\_data\_template\_info(2)}}\\
    uid &  name &  service &  service\_templ &  note\_host &  note\_host\_templ &  template\_host\\
    \hline
    \endhead
\label{tab:multi-reshostv2}
    2 & h2 & \{s1,s2\} & t2 &  &  & 2\\
    1 & h1 & \{s1,s2\} & t2 & note from t1 & t1 & 1\\
    \hline
\end{longtable}

\subsection{Roles and permissions}
We uses Postgresql roles to describe permissions to database objects (tables, functions etc.).
The {\tt deska\_user} role, is for user, running CLI (and Deska server). This role
has rights to execute all functions in json schema \footnote{These are all functions
that are called by Deska server, except {\tt restoringCommit}}.
So if you want to use CLI as normal user, you have to have you user in role {\tt deska\_user}
The second role is {\tt deska\_admin}, it own all object created during Deska database installation.
It can run {\tt restoringCommit}, so if you want to restore database, your user have to be in {\tt deska\_admin}.
Functions in Deska database are created with {\em "SECURITY DEFINER"}, that
specifies that the function is to be executed with the privileges of the user that created it.
That means, that json schema functions, created by {\tt deska\_admin} runs with its privilege,
and have access to all tables, meanwhile {\tt deska\_user} has not.

\label{sec:sql-procedures}

\section{SQL generator}
\label{sec:sql-generator}
This section describes how the tables and functions in the Deska database are generated. The Deska database is described in \secref{sec:deska-db}.\\
The SQL generator is run by Deska database intstallator. It generates stored procedures and tables that ensure data
manipulation for module tables from user defined scheme.\\

The generator is composed of classes {\tt Connection}, {\tt Schema}, {\tt Table}, {\tt Template}, {\tt Composition} and {\tt Multiref}.\\
The significant part of each class are string constants that represent plpgsql code with mapping keys. These string constants are formatted in class methods, mapping keys are replaced with appropriate parts of plpgsql code that could differ for various tables.
Detailed information about mentioned classes are below.

Parts of SQL generator run at the moment when all modules defined by user are already present as tables in the database. And they start in this order:

\begin{enumerate}
    \item {\tt Template} class - generate and create templates for tables that should have template
    \item {\tt Schema} class, {\tt Table} class - generate and create history tables for all tables (even for template tables that were generated)
    \item {\tt Schema} class, {\tt Table} class - generate and create stored procedures for data manipulation (the same stored procedures for tables defined by user and template tables)
    \item {\tt Multiref} class - generate and create tables and stored procedures for {\tt identifier\_set}s, history and template table for {\tt identifier\_set}s.
    \item {\tt Composition} class - generate and add triggers for {\tt COMPOSITION} relation
\end{enumerate}

\subsection{Schema}
The {\tt Schema} class controls generation of plpgsql code for creation of data manipulation functions. It is instantiated with {\tt Connection} parameter, which is used to query the database. 
The {\tt Schema} class finds out which kinds were created in database and what their structure is. The {\tt Schema} class instantiates {\tt Table} classes one by one for each kind defined by user. Appropriate methods of {\tt Table} class that generate plpgsql code according to kind's characteristics are then called.\\ It is necessary to distinguish between kinds that are {\tt EMBED INTO} (see \secref{sec:relation-embed-into}) and that are not. The part of data manipulation functions, where the concrete objects are identified and selected should be different for those {\tt EMBED INTO} and those not {\tt EMBED INTO} kinds.
For {\tt templated} kinds (\secref{sec:relation-templatized}) are in addition to not templated kinds generated functions for getting resolved data.\\
The final code for created kinds is generated inside the {\tt Table} class.

\subsection{Table}
A lot of string constants are present inside the {\tt Table} class. These string constants are prototypes of create table, create type statements, create function statements or some other pieces of plpgsql code.\\
Methods of {\tt Table} class return concrete plpgsql code for creating history tables and data manipulation functions. Generated functions are prepared to be created on the database server.\\

\begin{enumerate}
    \item get data functions
    \item add, delete objects, set attribute functions
    \item diff functions
    \item commit
\end{enumerate}

\subsubsection{History tables}
In following text we will talk about {\tt table} and its history table {\tt table\_history}.\\
For each table created in the schema production a table with suffix \_history is generated. {\tt table\_history} serves as complete history of objects of kind that is represented by {\tt table} in database. These history tables are created in the schema history.\\
{\tt table\_history} has the same attributes like {\tt table} and in addition {\tt dest\_bit} and {\tt version} attributes. {\tt dest\_bit} attribute indicates whether the object was deleted or not. In {\tt version} attribute is stored in which changeset was this row inserted. Inserted rows represents new object creation but even modification of an object. Don't confuse this {\tt version} attribute with the number of revision.

\subsubsection{Retrieve Object Data}
Getting objects' data means getting data of these objects' last modification before the given version in which user would like to get them.
In resolved modifications (resolving inherited values from object templates) of get data functions it is needed to resolve these data. And to resolve them again use inherited data from templates' last modification before the given version.
These resolving functions are written as recursive selects.

\subsubsection{Performing Modifications}
Create new object is done by inserting a row with values given {\tt name}, {\tt uid}, {\tt dest\_bit} set off and id of current changeset in {\tt version} attribute into history table.\\
Set attribute operation inserts current data of the object which we would like to modify into history table, sets value of {\tt version} attribute to id of current changeset and updates the value of attribute, which has to be set.\\
Object deletion is done by setting {\tt dest\_bit} attribute on.\\
The {\tt dest\_bit} attribute can be also set from on to off, this is the way, how we can undo deletion. But it is enabled only in the same changeset in which it was set to on. This deleted object will not exist later and will not be found.

\subsubsection{Diff}
For diffing are generated functions for diff initialization, processing data prepared during the initialization and cleaning after diff. For templated tables is generated also the resolved modification of function for diff initialization. Data processing stays the same even for diff of resolved data, because all data resolving is done in the initialization step.\\
These diff initialization functions have two forms, for getting list of modifications between two versions and between the state inside temporary changeset and in its parent version. Even data processing have to take into consideration between which stages is diffing done. It is because it is needed for columns, which refers to some table uid and were changed, to take the proper name.

\subsubsection{Commit}
Table class for given table generates also commit function that bring tables in the production schema up to data. As data in tables in the production schema are resolved and templated data could be modified even by a change in the template table, it is necessary to distinguish templated tables from not templated and generate different commit functions for them.

\subsection{Template}
When all tables described by user are created, tables that are marked as templated and have at least one attribute that could inherit its value are found. For these tables we generate template tables.\\
Template table generation means generation of create table statement for template table and modifications needed for having well defined template table.
Template table is created like table that will be templated by this template table. And thus created template table will have then dropped not null constraints and dropped columns that can't inherit their value.

\subsection{Composition}
{\tt Composition} class generates sql script that alters created history tables which are in some {\tt COMPOSITION} relation. These history tables are altered in the way to ensure consistent state of objects in composition relation. Triggers for linking objects and check constraints are added.\\
Linking objects of kinds in composition relation with same name is done by generating before and after insert and update triggers. These triggers ensure even disjoining if one of linked objects is renamed or deleted and can immediately join renamed object with another one.\\
Condition that one object could be contained in at most one another object is checked by the check constraints that are generated by the {\tt Composition} class as well.

\subsection{Multiref}
{\tt Multiref} class can for each {\tt identifier\_set} attribute of the given table generate create table statement for inner table, which provides {\tt Multi-value references} (see \secref{sec:relation-multi-value-references}). The generated inner class stores for object its {\tt identifier\_set}.\\
Generated inner table has always name with prefix "inner\_" followed by name of table, "\_" and name of table's {\tt identifier\_set} attribute. Inner tables have attributes attribute att1 with name of table tab1, attribute att2 with name of table tab2 that is referenced from {\tt identifier\_set} attribute of tab1, last attribute is a flag that is used to distinguish between rows representing empty set from rows representing NULL value. Generated inner table has following foreign key constraints, attribute att1 references uid column of table tab1 and attribute att2 references uid column of table tab2.\\
As inner table stores data of attribute that is part of versioned data, {\tt Multiref} class generates also create statement for history table of inner tables.\\ 
The table, to which attribute an inner table is made, could also be templated. Therefore {\tt Multiref} class generates create table statement for template table of these inner tables. And it also generates functions for resolving templated inner table's data.\\
In addition to ordinary set attribute function that is for each kind and its attribute generated by {\tt Table} class, {\tt Multiref} class generates functions that insert or remove identifier into or from identifier set.

\end{document}
