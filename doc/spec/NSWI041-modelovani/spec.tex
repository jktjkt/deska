\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english,czech]{babel}
\usepackage{a4wide}
\author{Jan KundrÃ¡t}
\title{Deska Scope Document}



\begin{document}

{\huge \textbf{Deska: Tool for Central Administration and Monitoring of a Grid Site}}

\vspace{0.5in}

\section{Background}

One of the activities taking at the Institute of Physics of the~AS~CR (FZU)~\cite{fzu} is running a local Tier-2 computing center
connected to an international WLCG computing grid~\cite{wlcg}, supporting user programs from various scientific communities from
the whole world including the D-0 experiment in Fermilab~\cite{d0} and the famous LHC accelerator from CERN~\cite{lhc}.

\section{Motivation}

In November 2009, the total installed computing capacity available to users was about 2000~CPU cores with more than 250~TB of
disk storage, which translates to several hundreds of physical machines.  All the resources are managed by just a few system
administrators (currently slightly more than three full time employees).  Due to the impressive machines-per-administrator ratio,
heavy emphasis is put in tasks aiming at increasing the effectiveness of certain processes and generally making the system
administrators' life easier and less error prone.

Over the years, certain monitoring and management systems were deployed at the Institute, including Cfengine~\cite{cfengine},
Nagios~\cite{nagios}, Ganglia~\cite{ganglia}, Munin~\cite{munin} and various in-house monitoring appliances.  While these tools
certainly aim to make the sysadmin's life easier, introduction of each new tool also adds one more place for the administrator to
check if she wants to make sure the infrastructure works as expected, and, in addition, each new tool has to be configured
separately.  In addition, each site in the WLCG Grid is also being monitored and checked by third parties.\footnote{Monitoring from
outside provides a highly needed perspective about site's availability from throughout the Grid.  Such external tools come in
variety of flavours, from simple tools checking whether certain values published in a global information system are sane to
complex solutions like automated job-submitting robots which monitor whether the site in question is in fact accepting jobs and if
these jobs terminate successfully in a timely manner.  Jobs running on site's worker nodes also have the ability to check for
various local settings like having fresh enough packages and fixed security vulnerabilities.}  Number of systems
monitoring a typical site from outside grows each year, introducing yet another burden on the Institute's staffers.  Therefore, it
has been decided that the Institute shall fund a development of a tool which will:

\begin{itemize}
    \item{automate the configuration process,}
    \item{aggregate the monitoring data at one place}
\end{itemize}

\section{System Requirements: Database}

The aim of the first part of the project is to develop a database storing generic description of all the resources that the Grid
site is using; such a database should be capable of describing hardware infrastructure of the data center\footnote{That is all
physical machines, their hardware models, part numbers, rack locations, network interconnects, history of hardware failures etc},
couplings of operating system instances (aka ``hosts'') to physical machines, logical relations between various
services\footnote{For example, each {\em subcluster} has one or more {\em gateways} called Computing Elements.  Failure of the CE,
as detected by upstream monitoring, might be actually caused by a dying hard drive on one single node in the cluster.  The system
must therefore be capable of storing such complex information.} as well as a history of issues encountered by each component.
This database shall then act as the single authoritative source of information for generating configuration of all components of
the system, including the monitoring appliances.  In addition, tools shall be written to perform consistency checks between the
state recorded in the database and the actual system shape when the automatic configuration generation is not feasible for safety
reasons\footnote{A classic example of such a system are network switches}.

Due to the Institute's administrators strong Unix background and on their explicit request, it is expected that the interface to
this database will be provided via text files instead of using a traditional web-based solutions.  The main reason for this design
choice is their familiarity with text-based configuration and good experience with text-processing tools in general.  Another
request of the staffers is the willing to store the generated configuration ``as close to the database as possible'' (for example
in the same revision control system) in order to guarantee the smallest deviation between the two sets possible.

The Database shall provide an API for read-only access to the data contained therein.  Modifications shall be performed only
through the developed user interface and not through the API for security reasons.  The whole system shall be able to scale up to
thousands of nodes on a commodity x86\_64 server hardware.

Furthermore, additional components for generating configuration on basis of actual system state, as retrieved through the
read-only database API, shall be developed for all systems which are currently deployed at FZU's site, namely for Nagios, BIND
name service demon, DHCP server and Ganglia and Munin master servers.  The design of the API should reflect generic needs of any
data center to allow eventual deployment of new components, should the need arise.  The API will also be used as a source of
information for the second part of the project, the Dashboard.

\section{System Requirements: Dashboard}

The second part of the system is the Dashboard, a tool whose purpose is to provide an accurate and up-to-date overview of the
state of the system.  The main philosophy behind this component is that there are already plenty of monitoring systems checking
the health and performance metrics of a grid site, yet no central place to look for troubles.  Therefore, it is pretty common that
a minor issue which is detected by only a single monitoring system gets unnoticed by the system administrators for several months.
The goal of the dashboard is, therefore, to provide a central view aggregating data from {\em all} the monitoring systems, both
local and remote.

The system shall be capable of storing the data provided by each monitoring system, performing a mapping from their naming
convention to an intermediate form, and grouping related sensors together.  Exploring ways in which to query various monitoring
systems for data is part of the design phase of the system.  The data shall be stored in format which makes it possible to view
history of each value up to a configurable age.  An example of such a backing store is the Round Robin Database (RRD)~\cite{rrd}.

The dashboard will aggregate and keep an archive of data coming from all the currently deployed monitoring systems.  A web portal
shall be developed depicting the health and the present state of a system in an intuitive way.  In addition to that, it shall be
possible for the user of the dashboard to view graphs depicting the history of various sensors, both in isolation and overlaid in
order to provide a way of figuring out correlations between different metrics\footnote{For example, system administrators might be
interested in figuring out if the network utilisation and the number of jobs currently running in the job system matches in order
to provide an infrastructure with balanced disk and CPU throughput}.  All the data shall be exportable to a format which makes it
easy to explore the data using a third-party statistical tool, but an elementary form of statistical analysis shall be integrated
into the core system.

Furthermore, due to a strong pressure to deploy Nagios as {\em the} monitoring solutions in the Grid, Nagios shall indeed be an
integral component of the design\footnote{For example, it shall be used for gathering data where possible, and detected alerts
shall be pushed back to Nagios for evaluation and passing them to the administrators}.

\section{Problems to Solve}

Monitoring hundreds or thousand of machines is a difficult task.  Problems that the team will have to solve are therefore mostly
related to scalability.  The RRDTool toolkit puts high load to the disk subsystem with its constant scattered writes.  The team
will have to investigate current solutions and come up with a solution which allows collecting data often enough that it is still
usable.  Care has to be taken to handle hosts suddenly not responding in a graceful way, so that they do not block the program
execution.

Another expected issue is developing pluggable modules for querying third-party monitoring systems.  It is to be explored if they
provide a reasonable API or if the team will have to resort to suboptimal means like parsing the HTML pages with overview.

\section{Future Improvements}

The system shall be extensible enough to be used outside of the Institute's computing infrastructure.  Areas for further
improvements include providing a plugin for analysis of the batch system controller, tracking data transfers from the disk
servers, improved visualization etc.

\section{TODO}

\begin{itemize}
    \item{lepe popsat, co vsechno bude dashboard delat}
    \item{zamyslet se nad ``ukladanim generovane konfigurace blizko databaze''}
    \item{jeste vic zduraznit mozne problemy?}
    \item{nezapomenout na vztah db -> dashboard a roli db pro vizualizaci}
    \item{priklady externich monitoringu -- clovek zvenku nevi, oc jde}
\end{itemize}

\begin{thebibliography}{9}
    \bibitem{fzu}Institute of Physics of the AS CR home page, {\tt http://www.fzu.cz/}
    \bibitem{wlcg}The W-LCG LHC Computing Grid, {\tt http://lcg.web.cern.ch/LCG/}
    \bibitem{d0}The DZero Experiment, {\tt http://www-d0.fnal.gov/}
    \bibitem{lhc}The Large Hadron Collider, {\tt http://lhc.web.cern.ch/lhc/}
    \bibitem{cfengine}Cfengine, {\tt http://www.cfengine.org/}
    \bibitem{nagios}Nagios, {\tt http://www.nagios.org/}
    \bibitem{ganglia}Ganglia, {\tt http://ganglia.sourceforge.net/}
    \bibitem{munin}Munin, {\tt http://munin.projects.linpro.no/}
    \bibitem{rrd}RRDTool, {\tt http://oss.oetiker.ch/rrdtool/}
\end{thebibliography}

\end{document}
