Information System for Managing a Grid Computing Center
-------------------------------------------------------

A draft specification by Jan Kundr√°t <kundratj@fzu.cz>

FIXME:
- Add more references
- Mention EGEE/EGI/WLCG/LHC
- Clarify roles of the HW DB


Motivation
----------

One of the activities taking at the Institute of Physics of the AS CR (FZU) is
running a local Tier-2 computing center connected to an international computing
grid, running user programs from various scientific communities from the whole
world.  Currently installed capacity of the clusters is about 1500 CPUs with more
than 250TB of disk storage.  The center itself consists of several hundreds of
physical machines.  While certain policies and management tools were developed
and deployed over the years, the center still lacks a centralised dashboard
listing the state of various resources.  The goal of this project is to fill
this gap.

Overview of already deployed tools
----------------------------------

The administrators were pretty successful at deploying various
industrial-standard management and monitoring tools. Nagios is used for fabric
monitoring, most of the machines' configuration is being handled by cfengine,
SNMP agents are used for monitoring critical parts of the network
infrastructure and Munin is used for generating graphs depicting performance
characteristics of the computing nodes.  Various in-house apps were developed to
assist with keeping track of HW issues and interesting events.

Unfortunately, all these tools are currently being managed in an isolated
manner. Despite some efforts, there is still no central place to define roles of
machines which would in turn be used by all the other tools.

Aim of the project
------------------

When the project is successfully finished and put into production use at the
Institute, we expect the following goals to be met:

- One single place to define services, their assignment to operating system
instances, mapping of OS instances to physical machines and the underlying
fabric scheme.  A cross-platform interface for editing of the information
contained therein shall be developed, too.

- One single place to look for troubles as detected by the monitoring tools
managed by the system.  A status dashboard aggregating results obtained by the
sensors.


Problems which will not be solved
---------------------------------

- While the team shall focus on proper modularisation of the system, they are
not expected to implement custom modules for services which are not currently
deployed at FZU

- Interaction with user-generated issues is specifically not part of the "single
place to look for troubles" requirement


Estimated plan of the implementation
------------------------------------

The very first goal of the team is to become familiar with the infrastructure
being used in FZU. The team shall focus on Nagios and Cfengine as well as
investigate design decisions taken by Quattor and Lemon projects.

It is expected that a central role in the design of the project would be
fulfilled by a database storing service definitions and relations among them.
Therefore, a proper attention for designing this database in a
proper way should be paid.

When the DB is stable, the team will develop "connectors", ie. software which
would actually use the information contained therein for interaction with
already deployed services. If the connectors are able to generate group
definitions for Cfengine and a complete Nagios configuration, both of which
accurately model the real situation, the first goal of the project will be
considered fulfilled.

The other use of the DB data is to provide a tactical overview of the overal
system health.  A web-based interface providing views based on both physical
placement (that is, rack view) and logical relations (hosts grouped together
based on what their desired role is) will be developed. This interface should be
tightly integrated with all the monitoring tools.


Design considerations
---------------------

We expect to reuse existing technologies and products whenever feasible. Due to
the increasing popularity of Nagios as the monitoring tool in the grid
community, it will likely be used as a central component of the design, even
to an extent that plugging a different monitoring tool would not be supported.

Ways for further automation of the system deployment will be investigated. In an
ideal case, it should be possible to just ask the system to "make the machine
XYZ a regular production webserver" and the system will take care of everything,
but in the real world, such a high integration is likely to prove troublesome.
Therefore, a correct balance between automation and reliability (which is
expected to increase when the human supervisor is not routed around) shall be
sought.


Possibility of future development
---------------------------------

It is expected that the system will be put into production use at FZU and
announced on related Grid symposiums. Ideas for future development might include
developing broader connectors for collaborating with different
implementations (like Puppet instead of Cfengine) and automated discovery of
topology and/or system roles. Various methods for verification that the real
topology matches the database data would be surely benefical, too. However, it
is important to stress enough that these extensions are strictly plans for
future development and not design goals of the first project's release.
