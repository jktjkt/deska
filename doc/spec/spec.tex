\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[czech,english]{babel}
\usepackage{a4wide}

\begin{document}

\title{Deska: A Tool for Central Administration and Monitoring of a Grid Site}

\author{Jan Kundrát \and Martina Krejčová \and Tomáš Hubík \and Lukáš Kerpl}

\maketitle

\section{Background}

One of the activities taking place at the Institute of Physics of the~AS~CR (FZU)~\cite{fzu} is running a local Tier-2 computing
center connected to an international WLCG computing grid~\cite{wlcg}, supporting user programs from various scientific communities
from the whole world including the D-0 experiment in Fermilab~\cite{d0} and the famous LHC accelerator from CERN~\cite{lhc}.

\section{Motivation}

In December 2010, the total installed computing capacity available to users was about 3000~CPU cores with more than 500~TB of disk
storage, which translates to several hundreds of physical machines, with more hardware to arrive in early 2011.  All the resources
are managed by just a few system administrators (currently slightly more than three full time employees).  Due to the impressive
machines-per-administrator ratio, the staffers are exploring ways about how to make the system administrators' life easier and
less error prone.

Over the years, certain monitoring and management systems were deployed at the Institute, including Cfengine~\cite{cfengine},
Nagios~\cite{nagios}, Ganglia~\cite{ganglia}, Munin~\cite{munin} and various in-house monitoring appliances.  While these tools
certainly aim to make the sysadmin's life easier, introduction of each new tool also adds one more place for the administrator to
check if she wants to make sure the infrastructure works as expected, and each new tool has to be configured separately.
Therefore, it has been decided that the Institute shall fund a development of a tool which will {\em automate the configuration
process}.

\section{System Requirements}

\subsection{Database}

The aim of the first part of the project is to develop a database storing generic description of all the resources that the Grid
site is using; such a database should be capable of describing hardware infrastructure of the data center\footnote{That is all
physical machines, their hardware models, part numbers, rack locations, network interconnects, history of hardware failures etc},
couplings of operating system instances (aka ``hosts'') to physical machines, logical relations between various
services\footnote{For example, each {\em subcluster} has one or more {\em gateways} called Computing Elements.  Failure of the CE,
as detected by upstream monitoring, might be actually caused by a dying hard drive on one single node in the cluster.  The system
must therefore be capable of storing such complex information.}.  This database shall then act as the single authoritative source
of information for generating configuration of all components of the system, including the monitoring appliances.  In addition,
tools shall be written to perform consistency checks between the state recorded in the database and the actual system shape when
the automatic configuration generation is not feasible for safety reasons\footnote{A classic example of such a system are network
switches -- system administrators are rather leery when it comes to automated configuration as a single error can cause large
network segments to separate.}.

Due to the Institute's administrators' strong Unix background and on their explicit request, it is expected that the interface to
this database will be provided via a CLI application instead of using a traditional web-based approach.  The main reason for this
design choice is their familiarity with text-based configuration tools and bad experience with web applications in general.  The
application interface shall be similar to a CLI-oriented interface used on enterprise-grade Ethernet switches, notable to the
Cisco IOS shell.  The format shall enable storing of plaintext dumps of the database in a version control system.  It is expected
that the CLI application will include ``wizards'' for common tasks like deploying a new machine in order to make the user's job as
smooth as possible.

The Database shall provide an API for access to the data contained therein.  User authentication and authorization shall be
handled by already existing operating system facilities (PAM, SSH).  The whole system shall be able to scale up to thousands of
nodes on a commodity x86\_64 server hardware.

\subsection{Add-on modules}

Additional components for generating configuration on basis of actual system state, as retrieved through the
read-only database API, shall be developed for all systems which are currently deployed at FZU's site, namely for Nagios, BIND
name service demon, DHCP server and Ganglia and Munin master servers.  The design of the API should reflect generic needs of any
data center to allow eventual deployment of new components, should the need arise.  The information available through the API
shall make it possible to generate an accurate visualization of the data center and all deployed systems, from the physical view
showing racks of servers to a high-level view depicting logical dependencies between services and machines.

Bindings to scripting languages like Python shall be introduced, as the staffers prefer writing their own configuration backends
in a language which allows faster prototyping than C++.  Therefore, the API shall offer a complete bindings for Python, along with
well documented sample modules.

\subsection{Management Utilities}

The information contained in the database shall be used for variety of other tasks, besides generating configuration files.  One
practical use case is creating a unified interface for console access to the physical machines.  This interface shall build on the
existing tools for handling the actual access (likely a SSH tunnel, a standard KVM access or perhaps a Java-based browser method),
but encapsulate all the details like physical location from the user.

A similar application is controlling the host power state; again, the goal here is not to reimplement the OpenIPMI implementation,
but hide various differences in the protocol spoken by various machines, automatically using correct credentials for access etc
etc.

\section{Problems to Solve}

On a first sight, the specification of an ``inventory management tool'' could be perceived as an easy task; that is, however, not
the case.  The biggest complication of the whole design is finding a right balance between usability and completeness.  The
Institute has had a web-based inventory management application capable of tracking various pieces of data for years, but the
staffers were hesitant to use it and, more importantly, depend on it for day-to-day operation.  As per an explicit request given
by the staffers, this project shall develop a console-based tool with strong focus on scripting, ease of function and the general
feeling of {\em helping} with getting things done instead of building artificial obstructions.

Given the always-changing nature of the data center operations, it is unlikely that a static database structure would accommodate
future developments{\footnote Indeed, many competing modules were struggling when servers with sizes different than several rack
units arrived}.  Therefore, the structure of the database shall not hard code any current assumptions about how a datacenter looks
like, but shall rather provide a way to describe generic collections of objects and their hierarchies.  Needless to say, such an
approach puts even bigger pressure on designing the system (and the database structure implemented at the Institute), as finding a
balance between being ``generic enough'' and ``usable enough'' is extremely complicated.

FIXME: versioning\ldots

The problem of database versioning and associated accounting is far from trivial, too, especially when combined with the fact that
the database structure is not set in stone in advance, but\ldots FIXME maybe talk about the dynamic stuff before?

FIXME, FIXME, FIXME.

FIXME: talk about security, reliability, scalability :),\ldots

The structure of the Database shall be thoroughly planned, too.  The team will have to balance the complexity needed for accurate
representation of the real infrastructure with the required level of usability.  The interface should be intuitive enough so that
the administrator will actually notice the improvements and start to like the system in favor of manual hacks.

\section{Future Improvements}

The system shall be extensible enough to be used outside of the Institute's computing infrastructure.  A nice feature would be
also a ``one-click'' tool for fully automated deployment of new machines, handling everything from setting up PXE environment for
installation to actual system configuration.


\begin{thebibliography}{9}
    \bibitem{fzu}Institute of Physics of the AS CR home page, {\tt http://www.fzu.cz/}
    \bibitem{wlcg}The W-LCG LHC Computing Grid, {\tt http://lcg.web.cern.ch/LCG/}
    \bibitem{d0}The DZero Experiment, {\tt http://www-d0.fnal.gov/}
    \bibitem{lhc}The Large Hadron Collider, {\tt http://lhc.web.cern.ch/lhc/}
    \bibitem{cfengine}Cfengine, {\tt http://www.cfengine.org/}
    \bibitem{nagios}Nagios, {\tt http://www.nagios.org/}
    \bibitem{ganglia}Ganglia, {\tt http://ganglia.sourceforge.net/}
    \bibitem{munin}Munin, {\tt http://munin.projects.linpro.no/}
\end{thebibliography}

\end{document}
